{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_KEY\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "token= os.getenv(\"IUCN_API_KEY\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langsmith import traceable\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"node node--type-article node--view-mode-full\",\n",
    "                                        \"span12\",\"gutter-horiz-in lead wysiwyg\", \" wysiwyg aside bordered\"))\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://wwf.be/fr/communiques-de-presse/rapport-planete-vivante-du-wwf-les-populations-danimaux-sauvages-connaissent\",\n",
    "               \"https://www.worldwildlife.org/pages/what-is-biodiversity\"\n",
    "               ),\n",
    "\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "pdf_paths = [\"report-iucn.pdf\",\"2024-012-En.pdf\",\"franco2006.pdf\",\"matias2013.pdf\"]\n",
    "pdf_docs = []\n",
    "for pdf_path in pdf_paths:\n",
    "    pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "    pdf_docs.extend(pdf_loader.load())\n",
    "\n",
    "all_docs = web_docs + pdf_docs\n",
    "# print(f\"Nombre de documents chargés: {len(web_docs)}\")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(all_docs)\n",
    "\n",
    "len(all_splits)\n",
    "len(all_splits[200].page_content)\n",
    "# all_splits[0].metadata\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "if not os.path.exists(\"faiss_index\"):\n",
    "    print(\"folder not found\")\n",
    "    vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "    local_index=FAISS.load_local(\"faiss_index\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\"),allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"folder found\")\n",
    "    local_index=FAISS.load_local(\"faiss_index\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\"),allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = local_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "\n",
    "# retrieved_docs = retriever.invoke(\"quel est le déclin moyen des populations d'animaux sauvages ?\")retriever = local_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"quel est le déclin moyen des populations d'animaux sauvages ?\")\n",
    "# print(retrieved_docs)\n",
    "q = \"quel est le déclin moyen des populations d'animaux sauvages ?\"\n",
    "def format_docs(documents):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "def rag_formatter(input):\n",
    "    docs = retriever.invoke(input)\n",
    "    formatted_docs = format_docs(docs)\n",
    "    return formatted_docs\n",
    "\n",
    "\n",
    "@tool\n",
    "def rag_tool(input: str):\n",
    "    \"\"\"\n",
    "    LangChain tool that utilizes a Retrieval-Augmented Generation (RAG) system to provide\n",
    "    fact-based answers by retrieving relevant documents from a knowledge base.\n",
    "\n",
    "    :param input: The user's query or input string.\n",
    "    :return: A string containing relevant retrieved information that complements the input query.\n",
    "    \"\"\"\n",
    "    return rag_formatter(input)\n",
    "\n",
    "RAG = rag_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_species_assessment(genus, species):\n",
    "    \n",
    "    # API connexion\n",
    "    conn = http.client.HTTPSConnection(\"api.iucnredlist.org\")\n",
    "    payload = ''\n",
    "    headers = {\n",
    "        'Authorization': token\n",
    "    }\n",
    "\n",
    "    # Step 1: Fetching the Assessment ID\n",
    "    conn.request(\"GET\", f\"/api/v4/taxa/scientific_name?genus_name={genus}&species_name={species}\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    json_data = data.decode(\"utf-8\")\n",
    "    parsed_data = json.loads(json_data)\n",
    "\n",
    "    if \"assessments\" in parsed_data and len(parsed_data[\"assessments\"]) > 0:\n",
    "        assessment_id = parsed_data[\"assessments\"][0][\"assessment_id\"]\n",
    "\n",
    "        # Étape 2 : Usage of the assessment_id to get the details\n",
    "        conn.request(\"GET\", f\"/api/v4/assessment/{assessment_id}\", payload, headers)\n",
    "        res = conn.getresponse()\n",
    "        data = res.read()\n",
    "        json_data = data.decode(\"utf-8\")\n",
    "        parsed_data = json.loads(json_data)\n",
    "\n",
    "        if \"taxon\" in parsed_data:\n",
    "            result = parsed_data[\"taxon\"]\n",
    "            racine = parsed_data\n",
    "            species_info = {\n",
    "                \"assessment_id\" : assessment_id,\n",
    "                \"Scientifique name\": result.get(\"scientific_name\", \"N/A\"),\n",
    "                \"Order\": result.get(\"order_name\", \"N/A\"),\n",
    "                \"Class\": result.get(\"class_name\", \"N/A\"),\n",
    "                \"Kingdom\": result.get(\"kingdom_name\", \"N/A\"),\n",
    "                \"Level of danger\": racine['red_list_category']['description'].get('en', \"N/A\"),\n",
    "                \"Population trend\": racine['population_trend']['description'].get('en', \"N/A\")\n",
    "            }\n",
    "\n",
    "            habitats = racine['documentation'].get('habitats', \"\")\n",
    "            threats = racine['documentation'].get('threats', \"\")\n",
    "            measures = racine['documentation'].get('measures', \"\")\n",
    "            geo = racine['documentation'].get('range', \"\")\n",
    "            \n",
    "            cleaned_habitats = BeautifulSoup(habitats, \"html.parser\").get_text()\n",
    "            cleaned_threats = BeautifulSoup(threats, \"html.parser\").get_text()\n",
    "            cleaned_measures = BeautifulSoup(measures, \"html.parser\").get_text()\n",
    "            cleaned_geo = BeautifulSoup(geo, \"html.parser\").get_text()\n",
    "\n",
    "            species_info[\"Habitat\"] = cleaned_habitats\n",
    "            species_info[\"Threats\"] = cleaned_threats\n",
    "            species_info[\"measures\"] = cleaned_measures\n",
    "            species_info[\"range\"] = cleaned_geo\n",
    "\n",
    "            return species_info\n",
    "\n",
    "    return {\"message\": \"No data found for this assessment ID.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def iucn_redlist_tool(genus: str, species: str):\n",
    "    \"\"\"\n",
    "    LangChain tool that uses the IUCN Redlist API to retrieve information about a species.\n",
    "    \n",
    "    :param genus: The genus of the species (e.g. Panthera)\n",
    "    :param species: The species name (e.g. leo)\n",
    "    :return: A dictionary containing information about the species\n",
    "    \"\"\"\n",
    "    return get_species_assessment(genus, species)\n",
    "\n",
    "redlist = iucn_redlist_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools and agent initialisation\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "search = TavilySearchResults(max_results=5,\n",
    "                             search_depth=\"advanced\",include_answer=True,\n",
    "                             include_raw_content=True,include_images=False)\n",
    "\n",
    "tools = [search,redlist, RAG]\n",
    "model = ChatOpenAI(api_key=api_key, model=\"gpt-4o-mini\")\n",
    "\n",
    "agent_executor = create_react_agent(model, tools,checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "template = \"\"\"Use the tools at your disposition to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use a few sentences maximum and keep the answer concise, \n",
    "but if asked to, you can use more sentences to widely reply to the question.\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", template), (\"user\", \"{question}\" )]\n",
    ")\n",
    "\n",
    "chain =  { \"question\": RunnablePassthrough()} | prompt_template | agent_executor \n",
    "\n",
    "\n",
    "@traceable\n",
    "def Ask_AI(user_input: str):\n",
    "    response = chain.invoke(user_input,config)\n",
    "    final_content = response[\"messages\"][-1].content\n",
    "    answer= textwrap.fill(final_content,width=100)\n",
    "    print(answer)\n",
    "    return response['messages']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = input(\"Do you have Questions ?\")\n",
    "Ask_AI(Input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
